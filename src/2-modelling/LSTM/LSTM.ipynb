{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/mnt/guanabana/raid/home/slomp006')\n",
    "\n",
    "# Upload created train and vali files to server\n",
    "traindf=pd.read_csv('Input/train20152018.csv')\n",
    "validf=pd.read_csv('Input/vali20152018.csv')\n",
    "\n",
    "print(traindf.columns.values,validf.columns.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X contained nan values:  tensor(False) new train X contains nan values:  tensor(False)\n",
      "Old shape of train X:  torch.Size([33405, 4, 27]) New shape:  torch.Size([33405, 4, 27])\n",
      "Train Y contained nan values:  tensor(False) new train Y contains nan values:  tensor(False)\n",
      "Old shape of train Y:  torch.Size([33405, 4, 7]) New shape:  torch.Size([33405, 4, 7]) \n",
      "\n",
      "vali X contained nan values:  tensor(False) new vali X contains nan values:  tensor(False)\n",
      "Old shape of vali X:  torch.Size([30489, 4, 27]) New shape:  torch.Size([30489, 4, 27])\n",
      "vali Y contained nan values:  tensor(False) new vali Y contains nan values:  tensor(False)\n",
      "Old shape of vali Y:  torch.Size([30489, 4, 7]) New shape:  torch.Size([30489, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# Input features (make sure they are existing columns in dfs)\n",
    "vars = ['x', 'y', 'b1_median', 'b2_median', 'b3_median',\n",
    "       'b4_median', 'b5_median', 'b6_median', 'b7_median', 'nbr_median',\n",
    "       'ndmi_median', 'ndvi_median', 'nbr_iqr', 'ndmi_iqr', 'ndvi_iqr',\n",
    "       'min', 'max', 'intercept', 'co', 'si', 'co2', 'si2', 'trend',\n",
    "       'phase1', 'amplitude1', 'phase2', 'amplitude2']\n",
    "\n",
    "# Output features (targets) (make sure they are existing columns in dfs)\n",
    "targets = ['bare', 'crops',\n",
    "       'grassland', 'shrub', 'tree', 'urban_built_up', 'water']\n",
    "\n",
    "# Create X (input features) in loop\n",
    "# vali is validation\n",
    "trainX_list = []\n",
    "valiX_list = []\n",
    "\n",
    "for colname in vars: \n",
    "  # Get column of feature\n",
    "  col_train = traindf[colname]\n",
    "  col_vali = validf[colname]\n",
    "\n",
    "  # Create train and vali tensor\n",
    "  var_train = torch.tensor(col_train.values).view(-1, 4, 1)\n",
    "  var_vali = torch.tensor(col_vali.values).view(-1, 4, 1)\n",
    "\n",
    "  # Convert to float32 (used in LSTM)\n",
    "  var_train = var_train.to(dtype=torch.float32)\n",
    "  var_vali = var_vali.to(dtype=torch.float32)\n",
    "\n",
    "  # Append to lists\n",
    "  trainX_list.append(var_train)\n",
    "  valiX_list.append(var_vali)\n",
    "\n",
    "# Do the same for Y (targets)\n",
    "trainY_list = []\n",
    "valiY_list = []\n",
    "\n",
    "for colname in targets: \n",
    "  # Get column of feature\n",
    "  col_train = traindf[colname]\n",
    "  col_vali = validf[colname]\n",
    "\n",
    "  # Create train and vali tensor\n",
    "  var_train = torch.tensor(col_train.values).view(-1, 4, 1)\n",
    "  var_vali = torch.tensor(col_vali.values).view(-1, 4, 1)\n",
    "\n",
    "  # Convert to float32 (used in LSTM)\n",
    "  var_train = var_train.to(dtype=torch.float32)\n",
    "  var_vali = var_vali.to(dtype=torch.float32)\n",
    "\n",
    "  # Append to lists\n",
    "  trainY_list.append(var_train)\n",
    "  valiY_list.append(var_vali)\n",
    "\n",
    "# We now have multiple features as tensors but we want them as one. So concatenate the tensors along the last dimension\n",
    "X_train = torch.cat(trainX_list, dim=2)\n",
    "X_vali = torch.cat(valiX_list, dim=2)\n",
    "Y_train = torch.cat(trainY_list, dim=2)\n",
    "Y_vali = torch.cat(valiY_list, dim=2)\n",
    "\n",
    "# Now check if any tensors contain nan values and if so remove them (entire matrix). LSTM will not work (correctly) with nan values.\n",
    "X_train_old = X_train\n",
    "X_vali_old = X_vali\n",
    "Y_train_old = Y_train\n",
    "Y_vali_old = Y_vali\n",
    "\n",
    "# create an empty mask\n",
    "mask = None\n",
    "\n",
    "# If X_train contains nan, apply mask to Y_train and vice versa. Same applies for X and Y_vali.\n",
    "if torch.isnan(X_train).any():\n",
    "    mask = torch.isnan(X_train).any(dim=1).any(dim=1)\n",
    "    X_train = X_train[~mask]\n",
    "    Y_train = Y_train[~mask]\n",
    "    #Y_train = torch.masked_select(Y_train, mask)\n",
    "if torch.isnan(Y_train).any():\n",
    "    mask = torch.isnan(Y_train).any(dim=1).any(dim=1) \n",
    "    Y_train = Y_train[~mask]\n",
    "    X_train = X_train[~mask]\n",
    "    X_train = X_train[~mask]\n",
    "if torch.isnan(X_vali).any():\n",
    "    mask = torch.isnan(X_vali).any(dim=1).any(dim=1) \n",
    "    X_vali = X_vali[~mask]\n",
    "    Y_vali = Y_vali[~mask]\n",
    "if torch.isnan(Y_vali).any():\n",
    "    mask = torch.isnan(Y_vali).any(dim=1).any(dim=1) \n",
    "    Y_vali = Y_vali[~mask]\n",
    "    X_vali = X_vali[~mask]\n",
    "\n",
    "# Check final tensors (including shape)\n",
    "print(\"Train X contained nan values: \", torch.isnan(X_train_old).any(), \"new train X contains nan values: \", torch.isnan(X_train).any())\n",
    "print(\"Old shape of train X: \", X_train_old.shape, \"New shape: \", X_train.shape)\n",
    "print(\"Train Y contained nan values: \", torch.isnan(Y_train_old).any(), \"new train Y contains nan values: \", torch.isnan(Y_train).any())\n",
    "print(\"Old shape of train Y: \", Y_train_old.shape, \"New shape: \", Y_train.shape, \"\\n\")\n",
    "\n",
    "print(\"vali X contained nan values: \", torch.isnan(X_vali_old).any(), \"new vali X contains nan values: \", torch.isnan(X_vali).any())\n",
    "print(\"Old shape of vali X: \", X_vali_old.shape, \"New shape: \", X_vali.shape)\n",
    "print(\"vali Y contained nan values: \", torch.isnan(Y_vali_old).any(), \"new vali Y contains nan values: \", torch.isnan(Y_vali).any())\n",
    "print(\"Old shape of vali Y: \", Y_vali_old.shape, \"New shape: \", Y_vali.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample \n",
    "Some classes are underrepresented, oversample those if wanted. Not done in this study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original samples: 33405 \n",
      "\n",
      "Number of original samples per class (where class is > 0): \n",
      " bare: 5404 crops: 7153 grassland: 22912 shrub: 16832 tree: 14673 urban_built_up: 2280 water: 3229\n",
      "Number of original samples per class (where class is == 100): \n",
      " bare: 362 crops: 2101 grassland: 4957 shrub: 668 tree: 1646 urban_built_up: 9 water: 1768 \n",
      "\n",
      "Number of samples after upsampling: 34296 \n",
      "\n",
      "Number of samples per class (where class is represented > 0): \n",
      " bare: 5404 crops: 7153 grassland: 22912 shrub: 16832 tree: 14673 urban_built_up: 3171 water: 3229\n",
      "Number of original samples per class (where class is == 100): \n",
      " bare: 362 crops: 2101 grassland: 4957 shrub: 668 tree: 1646 urban_built_up: 900 water: 1768\n"
     ]
    }
   ],
   "source": [
    "# Indices:   0   ,    1   ,       2    ,    3    ,   4  ,       5         ,    6\n",
    "targets = ['bare', 'crops', 'grassland', 'shrub', 'tree', 'urban_built_up', 'water']\n",
    "\n",
    "# Clone original data\n",
    "X_new, Y_new = X_train.clone(), Y_train.clone()\n",
    "\n",
    "# See length of original samples\n",
    "print(\"Number of original samples:\", len(X_train), \"\\n\")\n",
    "class_counts = [len(Y_new[Y_new[:, -1, i] != 0]) for i in range(len(targets))]\n",
    "class_counts_100 = [len(Y_new[Y_new[:, -1, i] == 100]) for i in range(len(targets))]\n",
    "print(\"Number of original samples per class (where class is > 0):\", \"\\n\", \n",
    "      *[f\"{targets[i]}: {class_counts[i]}\" for i in range(len(targets))])\n",
    "print(\"Number of original samples per class (where class is == 100):\", \"\\n\", \n",
    "      *[f\"{targets[i]}: {class_counts_100[i]}\" for i in range(len(targets))], \"\\n\")\n",
    "\n",
    "# Function to upsample classes where value is > 0 (thus every sample where class is represented)\n",
    "def upsample(X, Y, i, n):\n",
    "    idx = Y[:, -1, i] != 0 # -1 to account for one that is already in the data set\n",
    "    X_sub = X[idx].repeat(n-1, 1, 1)\n",
    "    Y_sub = Y[idx].repeat(n-1, 1, 1)\n",
    "    return X_sub, Y_sub\n",
    "\n",
    "# Function to only upsample classes where value = 100 (thus every sample where class is 100)\n",
    "def upsample100(X, Y, i, n):\n",
    "    idx = Y[:, -1, i] == 100\n",
    "    X_sub = X[idx].repeat(n-1, 1, 1)\n",
    "    Y_sub = Y[idx].repeat(n-1, 1, 1)\n",
    "    return X_sub, Y_sub\n",
    "\n",
    "# Upsample underrepresented classes with i = indice of class and n = number of times to duplicate samples\n",
    "bare_X, bare_Y = upsample(X_new, Y_new, 0, 2) # indice  = 0 (bare) and number of duplicates is 2\n",
    "crops_X, crops_Y = upsample(X_new, Y_new, 1, 2)\n",
    "urban_X, urban_Y = upsample(X_new, Y_new, 5, 5)\n",
    "water_X, water_Y = upsample(X_new, Y_new, 6, 3)\n",
    "\n",
    "# Upsample classes where value is 100\n",
    "shrub_X, shrub_Y = upsample100(X_new, Y_new, 3, 3)\n",
    "urban_100_X, urban_100_Y = upsample100(X_new, Y_new, 5, 100)\n",
    "\n",
    "# Create upsampled training data\n",
    "X_new = torch.cat([X_new, urban_100_X], dim=0)\n",
    "Y_new = torch.cat([Y_new, urban_100_Y], dim=0)\n",
    "\n",
    "# Print new lengths\n",
    "print(\"Number of samples after upsampling:\", len(X_new), \"\\n\")\n",
    "new_class_counts = [len(Y_new[Y_new[:, -1, i] != 0]) for i in range(len(targets))]\n",
    "new_class_counts_100 = [len(Y_new[Y_new[:, -1, i] == 100]) for i in range(len(targets))]\n",
    "print(\"Number of samples per class (where class is represented > 0):\", \"\\n\", \n",
    "      *[f\"{targets[i]}: {new_class_counts[i]}\" for i in range(len(targets))])\n",
    "print(\"Number of original samples per class (where class is == 100):\", \"\\n\", \n",
    "      *[f\"{targets[i]}: {new_class_counts_100[i]}\" for i in range(len(targets))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use X_train and Y_train for original training data\n",
    "# Use X_new and Y_new for upsampled training data\n",
    "\n",
    "# Calculate the mean and standard deviation of each feature in the training set\n",
    "X_mean = X_train.mean(dim=0)\n",
    "X_std = X_train.std(dim=0)\n",
    "\n",
    "# Standardize the training set\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "\n",
    "# Standardize the vali set using the mean and standard deviation of the training set\n",
    "X_vali = (X_vali - X_mean) / X_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 1.5685, -0.2151, -0.2440, -0.2889, -0.3230, -0.6298,  1.2649, -0.1705,\n",
      "         -0.6481,  1.5472,  1.2727,  1.5949, -0.8758, -0.8019, -0.8339,  2.3079,\n",
      "          1.0298,  0.1754,  0.0027,  0.0171, -0.0176, -0.0083, -0.1726, -0.1598,\n",
      "         -0.0375,  0.1209, -0.0495],\n",
      "        [ 1.5685, -0.2151, -0.2693, -0.3206, -0.3422, -0.6264,  0.9960, -0.3349,\n",
      "         -0.7346,  1.4954,  1.2586,  1.5799, -0.9808, -0.8102, -0.9286,  2.2818,\n",
      "          1.0254,  0.2799, -0.0101,  0.1086, -0.0606, -0.1098, -0.2769,  0.0045,\n",
      "         -0.1165,  0.1781, -0.2167],\n",
      "        [ 1.5685, -0.2151, -0.2837, -0.3259, -0.3480, -0.6331,  0.9654, -0.3672,\n",
      "         -0.7453,  1.4954,  1.2799,  1.5290, -0.9691, -0.8030, -0.9489,  2.2830,\n",
      "          0.8027, -0.1660,  0.0257,  0.0328, -0.0695,  0.0092,  0.1693, -0.1541,\n",
      "         -0.1696, -0.5514, -0.2224],\n",
      "        [ 1.5685, -0.2151, -0.2907, -0.3436, -0.3780, -0.6465,  0.8925, -0.4097,\n",
      "         -0.7470,  1.4603,  1.2661,  1.4723, -0.8987, -0.7998, -0.8161,  1.7660,\n",
      "          0.8008,  0.2289,  0.0357,  0.0441,  0.2740, -0.0238, -0.2252, -0.6997,\n",
      "         -0.0647,  1.6413, -0.0592]]), tensor([[ 0.,  0.,  0., 68., 32.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., 29., 63.,  0.,  8.],\n",
      "        [ 0.,  0.,  0., 29., 63.,  0.,  8.],\n",
      "        [ 0.,  0.,  0., 29., 63.,  0.,  8.]]))\n",
      "(tensor([[ 0.4107,  0.8725,  0.0604,  0.0971,  0.1192,  0.1334,  0.2008,  0.0384,\n",
      "          0.1476,  0.0288,  0.1393,  0.0854, -0.1841, -0.1138,  0.1460, -0.1501,\n",
      "          0.2929,  0.4025,  0.0655,  0.0071,  0.0662,  0.0799, -0.4028, -1.7805,\n",
      "         -0.0155, -1.2483,  0.0278],\n",
      "        [ 0.4107,  0.8725,  0.0460,  0.0714,  0.0921,  0.1327,  0.1977,  0.0691,\n",
      "          0.3067,  0.0200,  0.1265,  0.0837, -0.0719, -0.1692,  0.0815,  0.0127,\n",
      "         -0.1355,  0.0555,  0.1373, -0.0238,  0.3063,  0.7267, -0.0555,  2.0877,\n",
      "         -0.0566, -1.3738,  0.1199],\n",
      "        [ 0.4107,  0.8725,  0.0521,  0.0759,  0.0996,  0.1200,  0.1826,  0.0631,\n",
      "          0.2858,  0.0325,  0.1355,  0.0906,  0.1155,  0.0491,  0.2753,  0.0301,\n",
      "         -0.2428, -0.1582,  0.2383,  0.0424,  0.6351,  0.2272,  0.1582, -1.7026,\n",
      "         -0.0574, -1.3300,  0.1044],\n",
      "        [ 0.4107,  0.8725,  0.0421,  0.0556,  0.0613,  0.0773,  0.1595,  0.0420,\n",
      "          0.2073,  0.0422,  0.2896,  0.1516,  0.2773,  0.1591,  0.4396, -0.0426,\n",
      "         -0.1545, -0.0767,  0.1362,  0.0218,  0.4640,  0.1620,  0.0768, -1.6516,\n",
      "         -0.0377, -1.2415,  0.0697]]), tensor([[23., 55.,  0.,  0.,  0., 22.,  0.],\n",
      "        [23., 55.,  0.,  0.,  0., 22.,  0.],\n",
      "        [23., 55.,  0.,  0.,  0., 22.,  0.],\n",
      "        [23., 55.,  0.,  0.,  0., 22.,  0.]]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "train_size = int(0.9 * X_train.size(0)) # was 0.9\n",
    "test_size = X_train.size(0) - train_size\n",
    "\n",
    "# Training test data set\n",
    "dataset = TensorDataset(X_train, Y_train)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create the TensorDataset for the vali set\n",
    "vali_dataset = TensorDataset(X_vali, Y_vali)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(vali_dataset[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26724 6681\n"
     ]
    }
   ],
   "source": [
    "# See how many samples in train and test\n",
    "\n",
    "train_elements = 0\n",
    "test_elements = 0\n",
    "\n",
    "for element in train_dataset:\n",
    "    train_elements += 1\n",
    "    \n",
    "for element in test_dataset:\n",
    "    test_elements += 1\n",
    "    \n",
    "print(train_elements, test_elements)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and apply LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer: Adam, LR: 0.001, hidden size: 256, epochs = 100, batch size = 64, no activation (SmoothL1 loss)\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Bidirectional LSTM\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        # LSTM \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Activation\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        # Linear with Xavier Uniform\n",
    "        self.linear = nn.Linear(hidden_size*2, output_size) # x2 (bidirectional)\n",
    "#         init.xavier_uniform_(self.linear.weight)\n",
    "        \n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Put input through LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Put LSTM output through dropout layer (prevent overfitting)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "        # Apply activation function \n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Linear transform x to shape [batch size, sequence length = 4, output size = 7]\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Make sure output distribution sums to 100\n",
    "        x = self.softmax(x) * 100\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(input, target):\n",
    "    log_input_probs = torch.log(input)\n",
    "    neg_log_likelihood = -torch.sum(target * log_input_probs, dim=-1)\n",
    "    loss = torch.mean(neg_log_likelihood)\n",
    "    return loss\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# loss_fn = cross_entropy\n",
    "# loss_fn = nn.L1Loss()\n",
    "# loss_fn = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various custom loss functions (L1Loss in study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kldiv(input, target):\n",
    "    \n",
    "    dom_samples_idx = target[:, -1, -1] == 60\n",
    "    dom_input = input[dom_samples_idx]\n",
    "    dom_target = target[dom_samples_idx]\n",
    "    \n",
    "    kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")(torch.log(input), target)\n",
    "    \n",
    "    kldiv_dom_loss = torch.tensor(0.0)\n",
    "    if dom_input.numel() > 0:\n",
    "        kldiv_dom_loss = nn.KLDivLoss(reduction=\"batchmean\")(torch.log(dom_input), dom_target)\n",
    "        \n",
    "    if kldiv_dom_loss == 0:\n",
    "        return kldiv_loss\n",
    "    else:\n",
    "        return kldiv_dom_loss\n",
    "    \n",
    "    \n",
    "def customloss(input, target):\n",
    "    kldiv = nn.KLDivLoss(reduction=\"batchmean\")(torch.log(input), target)\n",
    "    \n",
    "    l1 = nn.L1Loss()(input, target)\n",
    "    \n",
    "    return kldiv*l1\n",
    "\n",
    "def customloss(input, target):\n",
    "    dom_samples_idx = target[:, -1, -1] > 70\n",
    "    dom_input = input[~dom_samples_idx]\n",
    "    dom_target = target[~dom_samples_idx]\n",
    "\n",
    "    if dom_input.numel() > 0:\n",
    "        kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")(torch.log(dom_input), dom_target)  \n",
    "    else:\n",
    "        kldiv_loss = nn.KLDivLoss(reduction=\"batchmean\")(torch.log(input), target)\n",
    "    \n",
    "    l1 = nn.L1Loss()(input, target)\n",
    "    \n",
    "    return (kldiv_loss/input.size(0))+l1\n",
    "       \n",
    "# loss_fn = customloss\n",
    "loss_fn = nn.L1Loss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/run model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 149/200, Best loss: 9.719 (obtained at epoch 144)\r"
     ]
    }
   ],
   "source": [
    "# Number of input and output features\n",
    "input_number = len(vars)\n",
    "output_number = len(targets)\n",
    "\n",
    "# Instantiate the model\n",
    "model = BiLSTMModel(input_size=input_number, hidden_size=512, output_size=output_number)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.003)\n",
    "\n",
    "# Create a DataLoader for the training set and validation sets (specify batch size)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True) # batch size 128\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "vali_loader = torch.utils.data.DataLoader(vali_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Make lists to retain loss\n",
    "losses_train_epochs = []\n",
    "losses_test_epochs = []\n",
    "\n",
    "# Retain prediction at best loss\n",
    "best_loss = float(\"inf\")\n",
    "best_pred = []\n",
    "best_epoch = 0\n",
    "\n",
    "# Epochs\n",
    "num_epochs = 200 # The higher, the longer it takes but the better idea of the model's fitting ability\n",
    "    \n",
    "# Loop over the training data\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Progress\n",
    "    print(\"\\rAt epoch: {}/{}, Best loss: {} (obtained at epoch {})\".format(epoch, \n",
    "                                                                            num_epochs, \n",
    "                                                                            round(best_loss, 3),\n",
    "                                                                            best_epoch), end='\\r')\n",
    "\n",
    "    # Loss per epoch\n",
    "    epoch_trainloss = []\n",
    "    epoch_testloss = []\n",
    "\n",
    "    # Predictions per epoch\n",
    "    epoch_pred = []\n",
    "    epoch_actual = []\n",
    "    \n",
    "    # Set the model to training mode  \n",
    "    model.train()\n",
    "\n",
    "    # Loop over the training set\n",
    "    for X, Y in train_loader:\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        Y_pred = model(X)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Append the loss to the lists\n",
    "        epoch_trainloss.append(loss.item())\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Loop over the test data set\n",
    "        for X, Y in test_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred = model(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(Y_pred, Y)\n",
    "\n",
    "            # Append the loss to the list\n",
    "            epoch_testloss.append(loss.item())\n",
    "        \n",
    "        # Make predictions\n",
    "        # Loop over the vali data set\n",
    "        for X, Y in vali_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred = model(X)\n",
    "\n",
    "            # Store predictions for batch\n",
    "            Y_pred_list = [x.tolist() for x in Y_pred]\n",
    "            Y_list = [x.tolist() for x in Y]\n",
    "\n",
    "            # Add batch prediction to list \n",
    "            epoch_pred.append(Y_pred_list)\n",
    "            epoch_actual.append(Y_list)\n",
    "\n",
    "    # Add epoch losses to total loss list \n",
    "    losses_train_epochs.append(epoch_trainloss)\n",
    "    losses_test_epochs.append(epoch_testloss)\n",
    "\n",
    "    # Check if the current epoch achieved the lowest observed loss, if so, save epoch prediction\n",
    "    if (sum(epoch_testloss) / len(test_loader)) < best_loss:\n",
    "        best_loss = sum(epoch_testloss) / len(test_loader)\n",
    "        best_pred = epoch_pred\n",
    "        actual = epoch_actual\n",
    "        best_epoch = epoch\n",
    "\n",
    "print(\"\\n\", \"Done\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See losses\n",
    "See how model fits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest loss: 9.69 is found at epoch: 199 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Average loss per epoch\n",
    "ltrain_epochs = []\n",
    "for epoch_list in losses_train_epochs:\n",
    "  epoch_avg = sum(epoch_list) / len(train_loader)\n",
    "  ltrain_epochs.append(epoch_avg)\n",
    "\n",
    "ltest_epochs = []\n",
    "for epoch_list in losses_test_epochs:\n",
    "  epoch_avg = sum(epoch_list) / len(test_loader)\n",
    "  ltest_epochs.append(epoch_avg)\n",
    "\n",
    "# Print epoch achieving minimum vali loss\n",
    "min_value = min(ltest_epochs)\n",
    "min_index = ltest_epochs.index(min_value)\n",
    "\n",
    "print(\"The lowest loss:\", round(min_value,2), \"is found at epoch:\", min_index, \"\\n\")\n",
    "\n",
    "# Plot the losses over time\n",
    "plt.plot(ltrain_epochs, label=\"Training loss\")\n",
    "plt.plot(ltest_epochs, label=\"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See (and check) predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Make lists of predictions and actual fractions (reference)\n",
    "nested_pred = best_pred # results from the epoch with the minimum loss is taken\n",
    "nested_actual = actual\n",
    "\n",
    "unnested_pred = []\n",
    "for data in nested_pred:\n",
    "  for batch in data:\n",
    "    for timestep in batch:\n",
    "      for target in range(len(timestep)):\n",
    "        unnested_pred.append(timestep[target])\n",
    "\n",
    "unnested_actual = []\n",
    "for data in nested_actual:\n",
    "  for batch in data:\n",
    "    for timestep in batch:\n",
    "      for target in range(len(timestep)):\n",
    "        unnested_actual.append(timestep[target])\n",
    "\n",
    "# These lists contain output for entire predictions \n",
    "# Now retain lists for each class\n",
    "pred_class = {}\n",
    "true_class = {}\n",
    "\n",
    "# Initialize lists for each class in predictions\n",
    "for i in range(len(targets)):\n",
    "  pred_class[f'{targets[i]}'] = unnested_pred[i::len(targets)]\n",
    "\n",
    "# Initialize lists for each class in reference data\n",
    "for i in range(len(targets)):\n",
    "  true_class[f'{targets[i]}'] = unnested_actual[i::len(targets)]\n",
    "\n",
    "RMSEavg = 0\n",
    "MAEavg = 0\n",
    "\n",
    "# Plot the lists as graphs\n",
    "\n",
    "# Loop through the data and plot the actual and predicted values\n",
    "for i in range(len(targets)):\n",
    "    # Get the data for the current class\n",
    "    actual = true_class[f'{targets[i]}']\n",
    "    predicted = pred_class[f'{targets[i]}']\n",
    "\n",
    "    # Define the x-axis data as a range of values from 0 to the length of the data\n",
    "    x = range(len(actual))\n",
    "\n",
    "    # Create a new figure\n",
    "    fig = plt.figure(i)\n",
    "\n",
    "    # Create a figure with certain size\n",
    "    fig = plt.figure(figsize=(20, 3))\n",
    "\n",
    "    # Create axes\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Plot the actual and predicted values\n",
    "    ax.plot(x, actual, label='Actual')\n",
    "    ax.plot(x, predicted, label='Predicted')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Set the title using the class name\n",
    "    var_name = f'{targets[i]}'\n",
    "    ax.set_title(var_name)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "    # Print RMSE / MAE\n",
    "    rmse = mean_squared_error(predicted, actual) ** 0.5\n",
    "    RMSEavg = RMSEavg + rmse\n",
    "    print(f'RMSE for {var_name}: {rmse}')\n",
    "\n",
    "    difference = [abs(predicted - actual) for predicted, actual in zip(predicted, actual)]\n",
    "    mae = mean(difference)\n",
    "    MAEavg = MAEavg + mae\n",
    "    print(f'MAE for {var_name}: {mae}')\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f'Average RMSE is {RMSEavg / len(targets)} and average MAE is {MAEavg / len(targets)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prediction data frame and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add IDs (vali data was not shuffled so can add like this --- checked with excel!)\n",
    "import pandas as pd\n",
    "pred_df = validf.loc[:, ['sample_id', 'location_id', 'validation_id', 'reference_year']]\n",
    "\n",
    "# Adds predictions to df \n",
    "for i in range(len(targets)):\n",
    "  data = pred_class[f'{targets[i]}']\n",
    "  pred_df[targets[i]] = data \n",
    "\n",
    "# Show df\n",
    "pred_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('Output/LSTM/LSTM_pred.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
