{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM post-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121956 121956\n",
      "118680 118680\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/mnt/guanabana/raid/home/slomp006')\n",
    "\n",
    "# Train and vali should have been uploaded to server\n",
    "# Reference data\n",
    "traindf = pd.read_csv('Input/train20152018.csv')\n",
    "validf = pd.read_csv('Input/vali20152018.csv')\n",
    "\n",
    "# Predictions (upload RF predictions also to server)\n",
    "LSTM_pred = pd.read_csv('Output/LSTM/LSTM_pred.csv')\n",
    "RF_pred2015 = pd.read_csv(\"Output/RF/RFpredict2015.csv\")\n",
    "RF_pred2016 = pd.read_csv(\"Output/RF/RFpredict2016.csv\")\n",
    "RF_pred2017 = pd.read_csv(\"Output/RF/RFpredict2017.csv\")\n",
    "RF_pred2018 = pd.read_csv(\"Output/RF/RFpredict2018.csv\")\n",
    "\n",
    "# Remove NaN\n",
    "LSTM_pred = LSTM_pred.dropna()\n",
    "RF_pred2015 = RF_pred2015.dropna()\n",
    "RF_pred2016 = RF_pred2016.dropna()\n",
    "RF_pred2017 = RF_pred2017.dropna()\n",
    "RF_pred2018 = RF_pred2018.dropna()\n",
    "\n",
    "# Prediction don't have the same samples. Use similar IDs.\n",
    "common_ids = set(RF_pred2015['sample_id']).intersection(RF_pred2016['sample_id'], RF_pred2017['sample_id'], RF_pred2018['sample_id'])\n",
    "\n",
    "# Filter the DataFrames to only include rows with common sample_id values\n",
    "RF_pred2015 = RF_pred2015[RF_pred2015['sample_id'].isin(common_ids)]\n",
    "RF_pred2016 = RF_pred2016[RF_pred2016['sample_id'].isin(common_ids)]\n",
    "RF_pred2017 = RF_pred2017[RF_pred2017['sample_id'].isin(common_ids)]\n",
    "RF_pred2018 = RF_pred2018[RF_pred2018['sample_id'].isin(common_ids)]\n",
    "\n",
    "# Add reference year column\n",
    "RF_pred2015['reference_year'] = 2015\n",
    "RF_pred2016['reference_year'] = 2016\n",
    "RF_pred2017['reference_year'] = 2017\n",
    "RF_pred2018['reference_year'] = 2018\n",
    "\n",
    "# Create one big RF prediction data frame\n",
    "RF_pred = pd.concat([RF_pred2015, RF_pred2016, RF_pred2017, RF_pred2018])\n",
    "RF_pred = RF_pred.sort_values(['sample_id', 'reference_year'])\n",
    "\n",
    "# Now also create validation separate for LSTM and RF\n",
    "common_LSTM = set(LSTM_pred['sample_id']).intersection(validf['sample_id'])\n",
    "common_RF = set(RF_pred['sample_id']).intersection(validf['sample_id'])\n",
    "\n",
    "LSTM_vali = validf[validf['sample_id'].isin(common_LSTM)]\n",
    "RF_vali = validf[validf['sample_id'].isin(common_RF)]\n",
    "\n",
    "print(len(LSTM_pred), len(LSTM_vali))\n",
    "print(len(RF_pred), len(RF_vali))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensors from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input variables contained nan values:  tensor(False)\n",
      "RF input variables contained nan values:  tensor(False)\n",
      "LSTM tensor has shape: torch.Size([30489, 4, 7]) LSTM with inputs has shape: torch.Size([30489, 4, 34])\n",
      "--> LSTM vali has shape: torch.Size([30489, 4, 7]) and respective ID has shape:  torch.Size([30489, 4, 3])\n",
      "RF tensor has shape: torch.Size([29670, 4, 7]) RF with inputs has shape: torch.Size([29670, 4, 34])\n",
      " --> RF vali has shape: torch.Size([29670, 4, 7]) and respective ID has shape:  torch.Size([29670, 4, 3]) \n",
      "\n",
      "Input columns for synthesized data contained nan values:  tensor(False)\n",
      "Training contained nan values:  tensor(False)\n",
      "Old shape of synth inputs tensor:  torch.Size([33405, 4, 27]) New shape:  torch.Size([33405, 4, 27])\n",
      "Old shape of training tensor:  torch.Size([33405, 4, 7]) New shape:  torch.Size([33405, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# Classes are targets\n",
    "targets = ['bare', 'crops',\n",
    "       'grassland', 'shrub', 'tree', 'urban_built_up', 'water']\n",
    "\n",
    "# Make synthetic data based on input variables\n",
    "inputs = ['x', 'y', 'b1_median', 'b2_median', 'b3_median',\n",
    "       'b4_median', 'b5_median', 'b6_median', 'b7_median', 'nbr_median',\n",
    "       'ndmi_median', 'ndvi_median', 'nbr_iqr', 'ndmi_iqr', 'ndvi_iqr',\n",
    "       'min', 'max', 'intercept', 'co', 'si', 'co2', 'si2', 'trend',\n",
    "       'phase1', 'amplitude1', 'phase2', 'amplitude2']\n",
    "\n",
    "# Also save IDs for later\n",
    "IDs = ['sample_id', 'location_id', 'reference_year']\n",
    "\n",
    "# Create lists\n",
    "LSTM_pred_list = []\n",
    "RF_pred_list = []\n",
    "\n",
    "# Validation\n",
    "LSTM_vali_list = []\n",
    "RF_vali_list = []\n",
    "\n",
    "# Training\n",
    "train_list = []\n",
    "\n",
    "for colname in targets: \n",
    "    # Get column of feature\n",
    "    col_LSTM = LSTM_pred[colname]\n",
    "    col_LSTM_vali = LSTM_vali[colname]\n",
    "    col_RF = RF_pred[colname]\n",
    "    col_RF_vali = RF_vali[colname]\n",
    "    col_training = traindf[colname]\n",
    "\n",
    "    # Create tensors with sequence lengths of 4\n",
    "    LSTM_tensor = torch.tensor(col_LSTM.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    LSTM_vali_tensor = torch.tensor(col_LSTM_vali.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    RF_tensor = torch.tensor(col_RF.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    RF_vali_tensor = torch.tensor(col_RF_vali.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    training_tensor = torch.tensor(col_training.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "\n",
    "    # Append to lists\n",
    "    LSTM_pred_list.append(LSTM_tensor)\n",
    "    LSTM_vali_list.append(LSTM_vali_tensor)\n",
    "    RF_pred_list.append(RF_tensor)\n",
    "    RF_vali_list.append(RF_vali_tensor)\n",
    "    train_list.append(training_tensor)\n",
    "    \n",
    "# We now each class as a separate tensor but we want them as one. So concatenate the tensors along the last dimension\n",
    "tensor_LSTM = torch.cat(LSTM_pred_list, dim=-1)\n",
    "tensor_LSTM_vali = torch.cat(LSTM_vali_list, dim=-1)\n",
    "tensor_RF = torch.cat(RF_pred_list, dim=-1)\n",
    "tensor_RF_vali = torch.cat(RF_vali_list, dim=-1)\n",
    "tensor_training = torch.cat(train_list, dim=-1)\n",
    "\n",
    "# Do the same for the input var for the synthetic data\n",
    "input_list = []\n",
    "\n",
    "# Also create inputs for LSTM and RF\n",
    "LSTM_input_list = []\n",
    "RF_input_list = []\n",
    "for colname in inputs:\n",
    "    col_var = traindf[colname]\n",
    "    col_LSTM = LSTM_vali[colname]\n",
    "    col_RF = RF_vali[colname]\n",
    "    \n",
    "    var_tensor = torch.tensor(col_var.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    LSTM_tensor = torch.tensor(col_LSTM.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    RF_tensor = torch.tensor(col_RF.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    \n",
    "    input_list.append(var_tensor)\n",
    "    LSTM_input_list.append(LSTM_tensor)\n",
    "    RF_input_list.append(RF_tensor)\n",
    "    \n",
    "tensor_inputs = torch.cat(input_list, dim=-1)\n",
    "LSTM_inputs = torch.cat(LSTM_input_list, dim=-1)\n",
    "RF_inputs = torch.cat(RF_input_list, dim=-1)\n",
    "\n",
    "# From LSTM and RF inputs, create a LSTM and RF tensor that also contains input data\n",
    "tensor_LSTM_inputs = torch.cat([LSTM_inputs, tensor_LSTM], dim=-1)\n",
    "tensor_RF_inputs = torch.cat([RF_inputs, tensor_RF], dim=-1)\n",
    "\n",
    "# And for IDs\n",
    "LSTM_ID_list = []\n",
    "RF_ID_list = []\n",
    "for colname in IDs:\n",
    "    ID_LSTM = LSTM_vali[colname]\n",
    "    ID_RF = RF_vali[colname]\n",
    "    ID_LSTM_tensor = torch.tensor(ID_LSTM.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    ID_RF_tensor = torch.tensor(ID_RF.values).view(-1, 4, 1).to(dtype=torch.float32)\n",
    "    LSTM_ID_list.append(ID_LSTM_tensor)\n",
    "    RF_ID_list.append(ID_RF_tensor)\n",
    "tensor_LSTM_IDs = torch.cat(LSTM_ID_list, dim=-1)\n",
    "tensor_RF_IDs = torch.cat(RF_ID_list, dim=-1)\n",
    "\n",
    "# Check for NaN tensors\n",
    "tensor_inputs_old = tensor_inputs\n",
    "tensor_training_old = tensor_training\n",
    "tensor_LSTM_inputs_old = tensor_LSTM_inputs\n",
    "tensor_RF_inputs_old = tensor_RF_inputs\n",
    "\n",
    "# create an empty mask\n",
    "mask = None\n",
    "\n",
    "# Remove for each tensor that need to be similar\n",
    "if torch.isnan(tensor_inputs).any():\n",
    "    mask = torch.isnan(tensor_inputs).any(dim=1).any(dim=1)\n",
    "    tensor_inputs = tensor_inputs[~mask]\n",
    "    tensor_training = tensor_training[~mask]\n",
    "if torch.isnan(tensor_training).any():\n",
    "    mask = torch.isnan(tensor_training).any(dim=1).any(dim=1) \n",
    "    tensor_training = tensor_training[~mask]\n",
    "    tensor_inputs = tensor_inputs[~mask]\n",
    "if torch.isnan(tensor_LSTM_inputs).any():\n",
    "    mask = torch.isnan(tensor_LSTM_inputs).any(dim=1).any(dim=1) \n",
    "    tensor_LSTM_inputs = tensor_LSTM_inputs[~mask]\n",
    "    tensor_LSTM_vali = tensor_LSTM_vali[~mask]\n",
    "    tensor_LSTM_IDs = tensor_LSTM_IDs[~mask]\n",
    "if torch.isnan(tensor_RF_inputs).any():\n",
    "    mask = torch.isnan(tensor_RF_inputs).any(dim=1).any(dim=1) \n",
    "    tensor_RF_inputs = tensor_RF_inputs[~mask]\n",
    "    tensor_RF_vali = tensor_RF_vali[~mask]\n",
    "    tensor_RF_IDs = tensor_RF_IDs[~mask]\n",
    "\n",
    "# Check final tensors (including shape)\n",
    "print(\"LSTM input variables contained nan values: \", torch.isnan(tensor_LSTM_inputs_old).any())\n",
    "print(\"RF input variables contained nan values: \", torch.isnan(tensor_RF_inputs_old).any())\n",
    "print(\"LSTM tensor has shape:\", tensor_LSTM.shape, \"LSTM with inputs has shape:\", tensor_LSTM_inputs.shape)\n",
    "print(\"--> LSTM vali has shape:\", tensor_LSTM_vali.shape, \"and respective ID has shape: \", tensor_LSTM_IDs.shape)\n",
    "print(\"RF tensor has shape:\", tensor_RF.shape, \"RF with inputs has shape:\", tensor_RF_inputs.shape)\n",
    "print(\" --> RF vali has shape:\", tensor_RF_vali.shape, \"and respective ID has shape: \", tensor_RF_IDs.shape, \"\\n\")\n",
    "print(\"Input columns for synthesized data contained nan values: \", torch.isnan(tensor_inputs_old).any())\n",
    "print(\"Training contained nan values: \", torch.isnan(tensor_training_old).any())\n",
    "print(\"Old shape of synth inputs tensor: \", tensor_inputs_old.shape, \"New shape: \", tensor_inputs.shape)\n",
    "print(\"Old shape of training tensor: \", tensor_training_old.shape, \"New shape: \", tensor_training.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversample (optional, not done in study) and normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.9344, -0.8156, -0.3054, -0.3625, -0.4349, -0.5741,  0.1083, -0.3256,\n",
      "         -0.5998,  0.7540,  0.4553,  0.9182, -0.6640, -0.5542, -0.7645,  1.5714,\n",
      "          0.5591,  0.1443,  0.0314,  0.0059, -0.0222, -0.0372, -0.1427,  2.0769,\n",
      "         -0.0409,  0.6845, -0.0307],\n",
      "        [-0.9344, -0.8156, -0.2916, -0.3491, -0.4099, -0.5583,  0.1062, -0.2354,\n",
      "         -0.6373,  0.7594,  0.4917,  0.9138, -0.8441, -0.6033, -0.7908,  1.4993,\n",
      "          0.7171, -0.5174,  0.0760,  0.1983, -0.0615, -0.2086,  0.5192, -1.4817,\n",
      "         -0.1120,  0.4149, -0.2026],\n",
      "        [-0.9344, -0.8156, -0.2866, -0.3513, -0.4002, -0.5610,  0.2632, -0.2294,\n",
      "         -0.6263,  0.8209,  0.6468,  0.9794, -0.7001, -0.6175, -0.7231,  1.5042,\n",
      "          0.7210, -0.4896,  0.1674,  0.0613, -0.0933, -0.0505,  0.4917, -1.5600,\n",
      "         -0.1079,  0.5392, -0.2111],\n",
      "        [-0.9344, -0.8156, -0.2894, -0.3658, -0.3991, -0.5712,  0.2272, -0.2256,\n",
      "         -0.6416,  0.8165,  0.5480,  0.9200, -0.7304, -0.5482, -0.7642,  0.5714,\n",
      "          0.7273,  0.8677,  0.1072, -0.0211,  0.2494, -0.1243, -0.8647,  1.8366,\n",
      "         -0.0501,  1.1973, -0.0061]]), tensor([[ 0.,  0.,  9., 50., 41.,  0.,  0.],\n",
      "        [ 0.,  0.,  9., 50., 41.,  0.,  0.],\n",
      "        [ 0.,  0.,  9., 50., 41.,  0.,  0.],\n",
      "        [ 0.,  0.,  9., 50., 41.,  0.,  0.]]))\n"
     ]
    }
   ],
   "source": [
    "# First upsample some urban cases as was done before\n",
    "# Function to upsample classes where value is > 0 (thus every sample where class is represented)\n",
    "def upsample(X, Y, i, n):\n",
    "    idx = Y[:, -1, i] != 0 # -1 to account for one that is already in the data set\n",
    "    X_sub = X[idx].repeat(n-1, 1, 1)\n",
    "    Y_sub = Y[idx].repeat(n-1, 1, 1)\n",
    "    return X_sub, Y_sub\n",
    "\n",
    "# Function to only upsample classes where value = 100 (thus every sample where class is 100)\n",
    "def upsample100(X, Y, i, n):\n",
    "    idx = Y[:, -1, i] == 100\n",
    "    X_sub = X[idx].repeat(n-1, 1, 1)\n",
    "    Y_sub = Y[idx].repeat(n-1, 1, 1)\n",
    "    return X_sub, Y_sub\n",
    "\n",
    "urban_X, urban_Y = upsample(tensor_inputs, tensor_training, 5, 5) # repeat indice 5 (urban) times 5 where it is represented\n",
    "urban_100_X, urban_100_Y = upsample100(tensor_inputs, tensor_training, 5, 100) # repeat indice 5 (urban) times 100 where it is represented with 100\n",
    "\n",
    "# Create upsampled training data\n",
    "X_train = torch.cat([tensor_inputs, urban_X, urban_100_X], dim=0)\n",
    "X_train = torch.cat([tensor_inputs], dim=0)\n",
    "X_train_old = X_train\n",
    "Y_train = torch.cat([tensor_training, urban_Y, urban_100_Y], dim=0)\n",
    "Y_train = torch.cat([tensor_training], dim=0)\n",
    "Y_train_old = Y_train\n",
    "\n",
    "# Normalise\n",
    "X_mean = X_train.mean(dim=0)\n",
    "X_std = X_train.std(dim=0)\n",
    "\n",
    "# Standardize the training set\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "\n",
    "# Create data set for training and for synth training\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple LSTM model to generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Model\n",
    "class SynthModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0):\n",
    "        super().__init__()\n",
    "        # LSTM \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Activation\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        # Linear with Xavier Uniform\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "#         init.xavier_uniform_(self.linear.weight)\n",
    "        \n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Put input through LSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Put LSTM output through dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply tanh activation function\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Linear transform x to shape [batch size, sequence length = 4, output size = 7]\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Make sure output distribution sums to 100\n",
    "        x = self.softmax(x) * 100\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 1/2, Best loss: 15.03 (obtained at epoch 0)\n",
      " Done. Result at epoch 1 is taken.\n"
     ]
    }
   ],
   "source": [
    "# Input size is number of input variables and output size is number of classes\n",
    "synthmodel = SynthModel(input_size=len(inputs), hidden_size=128, output_size=len(targets)) \n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(synthmodel.parameters(), lr=0.01, weight_decay=0.0)\n",
    "\n",
    "# Create a DataLoader for the training set and validation sets (specify batch size)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1028, shuffle=True) \n",
    "\n",
    "# Retain data of best loss\n",
    "best_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "# Epochs\n",
    "num_epochs = 2 # Was 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Progress\n",
    "    print(\"\\rAt epoch: {}/{}, Best loss: {} (obtained at epoch {})\".format(epoch, \n",
    "                                                                            num_epochs, \n",
    "                                                                            round(best_loss, 3),\n",
    "                                                                            best_epoch), end='\\r')\n",
    "    \n",
    "    # Store epoch results in list\n",
    "    epoch_pred = []\n",
    "    epoch_pred_tensor = []\n",
    "    epoch_actual = []\n",
    "    epoch_actual_tensor = []\n",
    "    epoch_loss = []\n",
    "    \n",
    "    # Retain input tensor for later\n",
    "    input_tensor = []\n",
    "    \n",
    "    # Loop over the training set\n",
    "    for X, Y in train_loader:\n",
    "\n",
    "        input_tensor.append(X)\n",
    "        \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        Y_pred = synthmodel(X)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        epoch_loss.append(loss)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Predictions per batch\n",
    "        Y_pred_list = [x.tolist() for x in Y_pred]\n",
    "        Y_list = [x.tolist() for x in Y]\n",
    "        \n",
    "        # Add batch prediction to list\n",
    "        epoch_pred.append(Y_pred_list)\n",
    "        epoch_actual.append(Y_list)\n",
    "        epoch_pred_tensor.append(Y_pred)\n",
    "        epoch_actual_tensor.append(Y)\n",
    "        \n",
    "    # Retain a tensor format for the Markov Chain    \n",
    "    epoch_pred_tensor = torch.cat(epoch_pred_tensor, dim=0)\n",
    "    epoch_actual_tensor = torch.cat(epoch_actual_tensor, dim=0)\n",
    "    \n",
    "    # Also retain input tensor\n",
    "    input_tensor = torch.cat(input_tensor, dim=0)\n",
    "    \n",
    "    avg_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    \n",
    "    # Retain prediction if best loss\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss.item()\n",
    "        best_pred = epoch_pred\n",
    "        best_actual = epoch_actual\n",
    "        best_epoch = epoch\n",
    "        best_pred_tensor = epoch_pred_tensor\n",
    "        best_actual_tensor = epoch_actual_tensor\n",
    "        best_input_tensor = input_tensor\n",
    "        \n",
    "print(\"\\n\", \"Done.\", \"Result at epoch\", best_epoch, \"is taken.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check synthesized (noise) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Make lists of predictions and actual fractions (reference)\n",
    "nested_pred = best_pred # results from the epoch with the minimum loss is taken\n",
    "nested_actual = best_actual\n",
    "\n",
    "unnested_pred = []\n",
    "for data in nested_pred:\n",
    "  for batch in data:\n",
    "    for timestep in batch:\n",
    "      for target in range(len(timestep)):\n",
    "        unnested_pred.append(timestep[target])\n",
    "\n",
    "unnested_actual = []\n",
    "for data in nested_actual:\n",
    "  for batch in data:\n",
    "    for timestep in batch:\n",
    "      for target in range(len(timestep)):\n",
    "        unnested_actual.append(timestep[target])\n",
    "\n",
    "# These lists contain output for entire predictions \n",
    "# Now retain lists for each class\n",
    "pred_class = {}\n",
    "true_class = {}\n",
    "\n",
    "# Initialize lists for each class in predictions\n",
    "for i in range(len(targets)):\n",
    "  pred_class[f'{targets[i]}'] = unnested_pred[i::len(targets)]\n",
    "\n",
    "# Initialize lists for each class in reference data\n",
    "for i in range(len(targets)):\n",
    "  true_class[f'{targets[i]}'] = unnested_actual[i::len(targets)]\n",
    "\n",
    "RMSEavg = 0\n",
    "MAEavg = 0\n",
    "\n",
    "# Plot the lists as graphs\n",
    "\n",
    "# Loop through the data and plot the actual and predicted values\n",
    "for i in range(len(targets)):\n",
    "    # Get the data for the current class\n",
    "    true = true_class[f'{targets[i]}']\n",
    "    predicted = pred_class[f'{targets[i]}']\n",
    "\n",
    "    # Define the x-axis data as a range of values from 0 to the length of the data\n",
    "    x = range(len(true))\n",
    "\n",
    "    # Create a new figure\n",
    "    fig = plt.figure(i)\n",
    "\n",
    "    # Create a figure with certain size\n",
    "    fig = plt.figure(figsize=(20, 3))\n",
    "\n",
    "    # Create axes\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Plot the actual and predicted values\n",
    "    ax.plot(x, true, label='Actual')\n",
    "    ax.plot(x, predicted, label='Synthesized')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Set the title using the class name\n",
    "    var_name = f'{targets[i]}'\n",
    "    ax.set_title(var_name)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "    # Print RMSE / MAE\n",
    "    rmse = mean_squared_error(predicted, true) ** 0.5\n",
    "    RMSEavg = RMSEavg + rmse\n",
    "    print(f'RMSE for {var_name}: {round(rmse, 2)}')\n",
    "\n",
    "    difference = [abs(predicted - true) for predicted, true in zip(predicted, true)]\n",
    "    mae = mean(difference)\n",
    "    MAEavg = MAEavg + mae\n",
    "    print(f'MAE for {var_name}: {round(mae, 2)}')\n",
    "\n",
    "print(\"\\n\")\n",
    "RMSEavg = RMSEavg / len(targets)\n",
    "MAEavg = MAEavg / len(targets)\n",
    "\n",
    "print(f'Average RMSE is {round(RMSEavg, 2)} and average MAE is {round(MAEavg, 2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33405, 4, 7]) torch.Size([33405, 4, 7]) torch.Size([33405, 4, 27]) \n",
      "\n",
      "X and Y without inputs: torch.Size([66810, 4, 7]) torch.Size([66810, 4, 7])\n",
      "X and Y with inputs: torch.Size([66810, 4, 34]) torch.Size([66810, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# Tensors\n",
    "synth_lcf = best_pred_tensor.detach()\n",
    "actual_lcf = best_actual_tensor.detach()\n",
    "input_vars = best_input_tensor.detach()\n",
    "\n",
    "# See shapes\n",
    "print(synth_lcf.shape, actual_lcf.shape, input_vars.shape, \"\\n\")\n",
    "\n",
    "# Create a data set with and without inputs\n",
    "training_X = torch.cat([synth_lcf, actual_lcf], dim=0)\n",
    "training_Y = torch.cat([actual_lcf, actual_lcf], dim=0)\n",
    "\n",
    "synth_plus_inputs = torch.cat([input_vars, synth_lcf], dim=-1)\n",
    "actual_plus_inputs = torch.cat([input_vars, actual_lcf], dim=-1)\n",
    "\n",
    "training_X_with_inputs = torch.cat([synth_plus_inputs, actual_plus_inputs], dim=0)\n",
    "training_Y_with_inputs = torch.cat([actual_lcf, actual_lcf], dim=0) # the same as training_Y but for clarity\n",
    "\n",
    "# See data set shapes\n",
    "print(\"X and Y without inputs:\", training_X.shape, training_Y.shape)\n",
    "print(\"X and Y with inputs:\", training_X_with_inputs.shape, training_Y_with_inputs.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversample change samples\n",
    "(skip if not preferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6490, 4, 7]) torch.Size([6490, 4, 34])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with all samples that have experienced change\n",
    "# Make sure X and Y correspond with each other\n",
    "changed_tensorsY_list = []\n",
    "changed_tensorsX_list = []\n",
    "\n",
    "for i in range(training_Y_with_inputs.size(0)): \n",
    "    for j in range(training_Y_with_inputs.size(1)-1):  \n",
    "        if torch.any(training_Y_with_inputs[i, j, :] != training_Y_with_inputs[i, j+1, :]):\n",
    "            \n",
    "            # Append Y change tensors\n",
    "            change_tensor = training_Y_with_inputs[i, :, :].unsqueeze(0)\n",
    "            changed_tensorsY_list.append(change_tensor)\n",
    "            \n",
    "            # Append X change tensors\n",
    "            change_tensor_input = training_X_with_inputs[i, :, :].unsqueeze(0)\n",
    "            changed_tensorsX_list.append(change_tensor_input)\n",
    "                   \n",
    "            break\n",
    "       \n",
    "changed_tensors = torch.cat(changed_tensorsY_list, dim=0)\n",
    "changed_tensors_input = torch.cat(changed_tensorsX_list, dim=0)\n",
    "\n",
    "print(changed_tensors.shape, changed_tensors_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64900, 4, 7]) torch.Size([64900, 4, 34]) \n",
      "\n",
      "New X and Y with inputs where change samples are repeated:\n",
      "torch.Size([66810, 4, 34]) torch.Size([66810, 4, 7])\n",
      "to\n",
      "torch.Size([131710, 4, 34]) torch.Size([131710, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# Repeat change tensors x amount of times and concat back to original training data\n",
    "changed_tensors_repeated = changed_tensors.repeat(10, 1, 1)\n",
    "changed_tensors_input_repeated = changed_tensors_input.repeat(10, 1, 1)\n",
    "\n",
    "print(changed_tensors_repeated.shape, changed_tensors_input_repeated.shape, \"\\n\")\n",
    "\n",
    "# Concatenate back\n",
    "training_X_with_inputs_oversampled = torch.cat([training_X_with_inputs, changed_tensors_input_repeated], dim=0)\n",
    "training_Y_with_inputs_oversampled = torch.cat([training_Y_with_inputs, changed_tensors_repeated], dim=0)\n",
    "\n",
    "print(\"New X and Y with inputs where change samples are repeated:\")\n",
    "print(training_X_with_inputs.shape, training_Y_with_inputs.shape)\n",
    "print(\"to\")\n",
    "print(training_X_with_inputs_oversampled.shape, training_Y_with_inputs_oversampled.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalise validation input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X has shape: torch.Size([66810, 4, 34]) and training Y has shape: torch.Size([66810, 4, 7])\n",
      "LSTM X has shape: torch.Size([30489, 4, 34]) and LSTM Y has shape: torch.Size([30489, 4, 7])\n",
      "RF X has shape: torch.Size([29670, 4, 34]) and RF Y has shape: torch.Size([29670, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# The way input tensors are set up:\n",
    "# tensor_LSTM_inputs : torch.cat([LSTM_inputs, tensor_LSTM], dim=-1)\n",
    "# tensor_RF_inputs : torch.cat([RF_inputs, tensor_RF], dim=-1)\n",
    "# This means that the last 7 columns represent the land cover classes\n",
    "\n",
    "# Split into inputs and land cover fractions\n",
    "LSTM_X_inputs = tensor_LSTM_inputs[:, :, :-len(targets)]\n",
    "RF_X_inputs = tensor_RF_inputs[:, :, :-len(targets)]\n",
    "\n",
    "LSTM_X_lcf = tensor_LSTM_inputs[:, :, -len(targets):]\n",
    "RF_X_lcf = tensor_RF_inputs[:, :, -len(targets):]\n",
    "\n",
    "# Normalise inputs (X_mean and X_std were defined from the initial training data)\n",
    "LSTM_X_inputs_normalised = (LSTM_X_inputs - X_mean) / X_std\n",
    "RF_X_inputs_normalised = (RF_X_inputs - X_mean) / X_std\n",
    "\n",
    "# Rejoin data (land cover fractions are thus not normalised)\n",
    "LSTM_X = torch.cat([LSTM_X_inputs_normalised, LSTM_X_lcf], dim=-1)\n",
    "RF_X = torch.cat([RF_X_inputs_normalised, RF_X_lcf], dim=-1)\n",
    "\n",
    "# Now do the same, but also normalise land cover fractions\n",
    "X_mean_all = training_X_with_inputs.mean(dim=0)\n",
    "X_std_all = training_X_with_inputs.std(dim=0)\n",
    "\n",
    "# Standardize the sets\n",
    "training_X_with_inputs_alln = (training_X_with_inputs - X_mean_all) / X_std_all\n",
    "training_X_with_inputs_truthonly_alln = (actual_plus_inputs - X_mean_all) / X_std_all\n",
    "synth_plus_inputs_alln = (synth_plus_inputs - X_mean_all) / X_std_all\n",
    "LSTM_X_alln = (LSTM_X - X_mean_all) / X_std_all\n",
    "RF_X_alln = (RF_X - X_mean_all) / X_std_all\n",
    "\n",
    "# Twice synth\n",
    "training_X_with_inputs_alln_2x = torch.cat([training_X_with_inputs_alln, training_X_with_inputs_alln], dim=0)\n",
    "synth_plus_inputs_alln_2x = torch.cat([synth_plus_inputs_alln, synth_plus_inputs_alln], dim=0)\n",
    "output_2x = torch.cat([training_Y_with_inputs, training_Y_with_inputs], dim=0)\n",
    "synth_output_2x = torch.cat([actual_lcf, actual_lcf], dim=0)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Training X has shape:\", training_X_with_inputs.shape, \"and training Y has shape:\", training_Y_with_inputs.shape)\n",
    "print(\"LSTM X has shape:\", LSTM_X.shape, \"and LSTM Y has shape:\", tensor_LSTM_vali.shape)\n",
    "print(\"RF X has shape:\", RF_X.shape, \"and RF Y has shape:\", tensor_RF_vali.shape)\n",
    "\n",
    "# Create data sets\n",
    "train_dataset = TensorDataset(training_X_with_inputs, training_Y_with_inputs)\n",
    "train_dataset_truthonly = TensorDataset(actual_plus_inputs, actual_lcf)\n",
    "train_dataset_alln = TensorDataset(training_X_with_inputs_alln, training_Y_with_inputs)\n",
    "train_dataset_alln_2x = TensorDataset(training_X_with_inputs_alln_2x, output_2x)\n",
    "train_dataset_truthonly_alln = TensorDataset(training_X_with_inputs_truthonly_alln, actual_lcf)\n",
    "\n",
    "train_dataset_onlyLCF = TensorDataset(actual_lcf, actual_lcf)\n",
    "train_dataset_onlyLCF_with_synth = TensorDataset(training_X, training_Y)\n",
    "\n",
    "train_dataset_without_truth = TensorDataset(synth_plus_inputs, actual_lcf)\n",
    "train_dataset_without_truth_alln = TensorDataset(synth_plus_inputs_alln, actual_lcf)\n",
    "train_dataset_without_truth_alln_2x = TensorDataset(synth_plus_inputs_alln_2x, synth_output_2x)\n",
    "train_dataset_synth_onlyLCF = TensorDataset(synth_lcf, actual_lcf)\n",
    "\n",
    "LSTM_dataset = TensorDataset(LSTM_X, tensor_LSTM_vali)\n",
    "LSTM_dataset_alln = TensorDataset(LSTM_X_alln, tensor_LSTM_vali)\n",
    "LSTM_dataset_onlyLCF = TensorDataset(tensor_LSTM, tensor_LSTM_vali)\n",
    "\n",
    "RF_dataset = TensorDataset(RF_X, tensor_RF_vali)\n",
    "RF_dataset_alln = TensorDataset(RF_X_alln, tensor_RF_vali)\n",
    "RF_dataset_onlyLCF = TensorDataset(tensor_RF, tensor_RF_vali)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Bidirectional LSTM (full sequence input)\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Activation\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Put inputs through LSTM layer \n",
    "        x, _ = self.lstm(x) # [batch size, 4, hidden size]\n",
    "        hidden = x\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Go through final fully connected layer\n",
    "        x = self.fc(x) # [batch size, 4, 7]\n",
    "        \n",
    "        # Make sure output distribution sums to 100\n",
    "        x = self.softmax(x) * 100\n",
    "        \n",
    "        # Return both output and hidden layer (test what works better)\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists to retain loss\n",
    "losses_train_epochs = []\n",
    "losses_test_epochs = []\n",
    "\n",
    "# But retain only the best prediction (to save memory)\n",
    "best_loss = float(\"inf\")\n",
    "current_loss = float(\"inf\") \n",
    "truth_mae = float(\"inf\") \n",
    "best_pred = []\n",
    "best_epoch = 0\n",
    "best_layer = 0\n",
    "\n",
    "# Model tested:\n",
    "modeltype = \"RF\" # \"LSTM\" or \"RF\"\n",
    "\n",
    "# Number of stacked layers (just break operation otherwise)\n",
    "# In this study 2 were used (PostLSTM-1, PostLSTM-2), but one may consider to use more\n",
    "for layer in range(2):\n",
    "    \n",
    "    # Stop if previous layer didn't find better loss\n",
    "    if layer - best_layer > 1:\n",
    "        break\n",
    "\n",
    "    # At the start, initialise with either LSTM or RF\n",
    "    if layer == 0:\n",
    "        \n",
    "        if modeltype == \"LSTM\":\n",
    "            \n",
    "            # Or use synth with actual data as start data set\n",
    "            X_dataset = train_dataset_alln\n",
    "            Y_dataset = LSTM_dataset_alln\n",
    "            \n",
    "            train_size = int(0.9 * training_X_with_inputs_alln.size(0)) # was 0.5\n",
    "            test_size = training_X_with_inputs_alln.size(0) - train_size\n",
    "            X_dataset, test_dataset = random_split(X_dataset, [train_size, test_size])\n",
    "            \n",
    "            # Input length to start with (only vars)\n",
    "            start_input_length = training_X_with_inputs_alln.size(-1)\n",
    "            \n",
    "            # Or input length to start with (vars + predicted lcf)\n",
    "            start_input_length = training_X_with_inputs_alln.size(-1)\n",
    "        \n",
    "        elif modeltype == \"RF\":\n",
    "            # Start data sets\n",
    "            X_dataset = train_dataset_alln\n",
    "            \n",
    "            train_size = int(0.9 * training_X_with_inputs_alln.size(0)) # was 0.5\n",
    "            test_size = training_X_with_inputs_alln.size(0) - train_size\n",
    "            X_dataset, test_dataset = random_split(X_dataset, [train_size, test_size])\n",
    "            \n",
    "            Y_dataset = RF_dataset_alln\n",
    "            \n",
    "            # Input length to start with (vars + predicted lcf)\n",
    "            start_input_length = training_X_with_inputs_alln.size(-1)\n",
    "            \n",
    "        else:\n",
    "            print(\"No valid model input.\")\n",
    "            break\n",
    "\n",
    "        # Initialise model, optimiser and loss function    \n",
    "        loss_fn = nn.L1Loss()\n",
    "        model = BiLSTMModel(input_size = start_input_length, hidden_size = 256, output_size = 7) # hidden was 256\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001) # was 0.0003\n",
    "        \n",
    "        # Create a DataLoader for the training set and validation sets (specify batch size)\n",
    "        train_loader = torch.utils.data.DataLoader(X_dataset, batch_size=128, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "        vali_loader = torch.utils.data.DataLoader(Y_dataset, batch_size=128, shuffle=False) \n",
    "\n",
    "        # Epochs (LSTM may need longer at start)\n",
    "        if modeltype == \"LSTM\":\n",
    "            num_epochs = 50\n",
    "        else:\n",
    "            num_epochs = 50\n",
    "        \n",
    "        # Set the model to training mode \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # At the next layers\n",
    "        X_dataset = new_train_dataset\n",
    "        Y_dataset = new_vali_dataset \n",
    "        \n",
    "        train_size = int(0.9 * new_input_size) # was 0.9\n",
    "        test_size = new_input_size - train_size\n",
    "        X_dataset, test_dataset = random_split(X_dataset, [train_size, test_size])\n",
    "        \n",
    "        # Create a DataLoader for the training set and validation sets (specify batch size)\n",
    "        train_loader = torch.utils.data.DataLoader(X_dataset, batch_size=128, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "        vali_loader = torch.utils.data.DataLoader(Y_dataset, batch_size=128, shuffle=False) \n",
    "        \n",
    "        # Epochs\n",
    "        num_epochs = 150\n",
    "        \n",
    "        # Re-initialise model and remove gradients from previous layer\n",
    "        model = BiLSTMModel(input_size = new_input_length, hidden_size = 256, output_size = 7) \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Loop over the training data\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Progress\n",
    "        print(\"\\rAt epoch: {}/{}, Loss: {}, Best loss: {} (obtained at epoch {}, layer {})\".format(epoch, \n",
    "                                                                                num_epochs, round(current_loss, 3),\n",
    "                                                                                round(best_loss, 3),\n",
    "                                                                                best_epoch, best_layer+1), \n",
    "                                                                                end='\\r')\n",
    "\n",
    "        # Loss per epoch\n",
    "        epoch_trainloss = []\n",
    "        epoch_testloss = []\n",
    "\n",
    "        # Predictions per epoch\n",
    "        epoch_pred = []\n",
    "        epoch_pred_tensor = []\n",
    "        epoch_actual = []\n",
    "        epoch_actual_tensor = []\n",
    "\n",
    "        # Save hidden training input\n",
    "        hidden_train_input = []\n",
    "        \n",
    "        # Save pred training input\n",
    "        pred_train_input = []\n",
    "        \n",
    "        # Save train target\n",
    "        train_target = []\n",
    "        \n",
    "        # Start model training\n",
    "        model.train() \n",
    "        \n",
    "        # Loop over the training set\n",
    "        for X, Y in train_loader:\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the loss\n",
    "            Y_pred, Y_hidden = model(X)\n",
    "            loss = loss_fn(Y_pred, Y)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Append the loss to the lists\n",
    "            epoch_trainloss.append(loss.item())\n",
    "\n",
    "            # Save (input + prediction) + hidden\n",
    "            hidden_input = torch.cat([Y_hidden], dim=-1)\n",
    "            pred_input = torch.cat([X, Y_pred], dim=-1)\n",
    "            hidden_train_input.append(hidden_input)\n",
    "            pred_train_input.append(pred_input)\n",
    "            train_target.append(Y)\n",
    "\n",
    "        # Save hidden test/vali input\n",
    "        hidden_test_input = []\n",
    "        hidden_vali_input = []\n",
    "        \n",
    "        # Save pred test/vali input\n",
    "        pred_test_input = []\n",
    "        pred_vali_input = []\n",
    "        \n",
    "        # Save test/vali target\n",
    "        test_target = []\n",
    "        vali_target = []\n",
    "\n",
    "        with torch.no_grad(): \n",
    "\n",
    "            # Set the model to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Loop over the test data set\n",
    "            for X, Y in test_loader:\n",
    "\n",
    "                # Compute the loss     \n",
    "                Y_pred, Y_hidden = model(X)\n",
    "                loss = loss_fn(Y_pred, Y)\n",
    "\n",
    "                # Append the loss to the list\n",
    "                epoch_testloss.append(loss.item())\n",
    "\n",
    "                # Save (input + prediction) + hidden\n",
    "                hidden_input = torch.cat([Y_hidden], dim=-1)\n",
    "                pred_input = torch.cat([X, Y_pred], dim=-1)\n",
    "                hidden_test_input.append(hidden_input)\n",
    "                pred_test_input.append(pred_input)\n",
    "                test_target.append(Y)\n",
    "            \n",
    "            # Make predictions\n",
    "            # Loop over the vali data set\n",
    "            for X, Y in vali_loader:\n",
    "\n",
    "                # Get predictions    \n",
    "                Y_pred, Y_hidden = model(X)\n",
    "\n",
    "                # Store predictions for batch\n",
    "                Y_pred_list = [x.tolist() for x in Y_pred]\n",
    "                Y_list = [x.tolist() for x in Y]\n",
    "\n",
    "                # Add batch prediction to list \n",
    "                epoch_pred.append(Y_pred_list)\n",
    "                epoch_pred_tensor.append(Y_pred)\n",
    "                epoch_actual.append(Y_list)\n",
    "                epoch_actual_tensor.append(Y)\n",
    "\n",
    "                # Save (input + prediction) + hidden\n",
    "                hidden_input = torch.cat([Y_hidden], dim=-1)\n",
    "                pred_input = torch.cat([X, Y_pred], dim=-1)\n",
    "                hidden_vali_input.append(hidden_input)\n",
    "                pred_vali_input.append(pred_input)\n",
    "                vali_target.append(Y)\n",
    "\n",
    "        # Retain tensor\n",
    "        epoch_pred_tensor = torch.cat(epoch_pred_tensor, dim=0)\n",
    "        epoch_actual_tensor = torch.cat(epoch_actual_tensor, dim=0)\n",
    "\n",
    "        # Add epoch losses to total loss list \n",
    "        losses_train_epochs.append(epoch_trainloss)\n",
    "        losses_test_epochs.append(epoch_testloss)\n",
    "\n",
    "        # Retain new inputs/targets\n",
    "        # Training\n",
    "        hidden_train_input = torch.cat(hidden_train_input, dim=0)\n",
    "        pred_train_input = torch.cat(pred_train_input, dim=0)\n",
    "        train_target = torch.cat(train_target, dim=0)\n",
    "        \n",
    "        # Test\n",
    "        hidden_test_input = torch.cat(hidden_test_input, dim=0)\n",
    "        pred_test_input = torch.cat(pred_test_input, dim=0)\n",
    "        test_target = torch.cat(test_target, dim=0)\n",
    "        \n",
    "        # Vali\n",
    "        hidden_vali_input = torch.cat(hidden_vali_input, dim=0)\n",
    "        pred_vali_input = torch.cat(pred_vali_input, dim=0)\n",
    "        vali_target = torch.cat(vali_target, dim=0)\n",
    "        \n",
    "        # Current loss\n",
    "        current_loss = sum(epoch_testloss) / len(test_loader)\n",
    "        \n",
    "        if epoch == 0 and (layer == 1 or layer == 2 or layer == 3 or layer == 4 or layer == 5):\n",
    "            best_loss = current_loss\n",
    "        \n",
    "        # Check if the current epoch achieved the lowest observed loss, if so, save epoch prediction\n",
    "        if current_loss < best_loss:\n",
    "            \n",
    "            # Update\n",
    "            best_loss = current_loss\n",
    "            best_pred = epoch_pred\n",
    "            best_actual = epoch_actual\n",
    "            best_epoch = epoch\n",
    "\n",
    "            # Save prediction tensors\n",
    "            # Training\n",
    "            best_train_hidden = hidden_train_input\n",
    "            best_train_pred = pred_train_input\n",
    "            best_train_target = train_target\n",
    "            \n",
    "            # Test\n",
    "            best_test_hidden = hidden_test_input\n",
    "            best_test_pred = pred_test_input\n",
    "            best_test_target = test_target\n",
    "            \n",
    "            # Validation\n",
    "            best_vali_hidden = hidden_vali_input\n",
    "            best_vali_pred = pred_vali_input\n",
    "            best_vali_target = vali_target\n",
    "            \n",
    "            # Detach from graph for next iteration\n",
    "            # Training\n",
    "            best_train_hidden = best_train_hidden.detach()\n",
    "            best_train_pred = best_train_pred.detach()\n",
    "            best_train_target = best_train_target.detach()\n",
    "            \n",
    "            # Test\n",
    "            best_test_hidden = best_test_hidden.detach()\n",
    "            best_test_pred = best_test_pred.detach()\n",
    "            best_test_target = best_test_target.detach()\n",
    "            best_traintest_hidden = torch.cat([best_train_hidden, best_test_hidden], dim=0)\n",
    "            best_traintest_target = torch.cat([best_train_target, best_test_target], dim=0)\n",
    "            \n",
    "            # Validation\n",
    "            best_vali_hidden = best_vali_hidden.detach()\n",
    "            best_vali_pred = best_vali_pred.detach() \n",
    "            best_vali_target = best_vali_target.detach()\n",
    "            \n",
    "            # Normalise\n",
    "            train_mean = best_train_pred.mean(dim=0)\n",
    "            train_std = best_train_pred.std(dim=0)\n",
    "            best_train_pred = (best_train_pred - train_mean) / train_std\n",
    "            best_test_pred = (best_test_pred - train_mean) / train_std\n",
    "            best_vali_pred = (best_vali_pred - train_mean) / train_std\n",
    "            \n",
    "            # Hidden + lcf\n",
    "            predict_lcf = best_vali_pred[:, :, -len(targets):]\n",
    "            train_lcf = best_train_pred[:, :, -len(targets):]\n",
    "            vali_hidden_pred = torch.cat([best_vali_hidden, predict_lcf], dim=-1)\n",
    "            train_hidden_pred = torch.cat([best_train_hidden, train_lcf], dim=-1)\n",
    "            \n",
    "            # Pred of only this layer\n",
    "            if modeltype == \"LSTM\" and best_train_pred.size(0) < 35:\n",
    "                train_new_input = best_train_pred[:, :, :-7] # 7 because independent LSTM didn't contain previous predictions\n",
    "                predict_new_input = best_vali_pred[:, :, :-7]\n",
    "            else:    \n",
    "                train_new_input = best_train_pred[:, :, :-14] # 14 because contains two LCF predictions\n",
    "                test_new_input = best_test_pred[:, :, :-14]\n",
    "                predict_new_input = best_vali_pred[:, :, :-14]\n",
    "                \n",
    "            predict_lcf = best_vali_pred[:, :, -len(targets):] # only the last is the newest prediction\n",
    "            train_lcf = best_train_pred[:, :, -len(targets):]\n",
    "            test_lcf = best_test_pred[:, :, -len(targets):]\n",
    "            best_train_last_pred = torch.cat([train_new_input, train_lcf], dim=-1)\n",
    "            best_test_last_pred = torch.cat([test_new_input, test_lcf], dim=-1)\n",
    "            best_vali_last_pred = torch.cat([predict_new_input, predict_lcf], dim=-1)\n",
    "            \n",
    "            # Hidden + var + lcf\n",
    "            train_hidden_var_lcf = torch.cat([best_train_hidden, best_train_pred], dim=-1)\n",
    "            vali_hidden_var_lcf = torch.cat([best_vali_hidden, best_vali_pred], dim=-1)\n",
    "            \n",
    "            # Create data sets\n",
    "            # Combine train with test\n",
    "            best_traintest_hidden = torch.cat([best_train_hidden, best_test_hidden], dim=0)\n",
    "            best_traintest_input = torch.cat([best_train_last_pred, best_test_last_pred], dim=0)\n",
    "            best_traintest_target = torch.cat([best_train_target, best_test_target], dim=0)\n",
    "            \n",
    "            new_train_dataset = TensorDataset(best_traintest_input, best_traintest_target)\n",
    "            new_vali_dataset = TensorDataset(best_vali_last_pred, best_vali_target)\n",
    "            new_input_length = best_traintest_input.size(-1)\n",
    "            new_input_size = best_traintest_input.size(0)\n",
    "            \n",
    "            # Best layer\n",
    "            best_layer = layer\n",
    "        \n",
    "print(\"\\n\", \"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest loss: 5.158 is found at epoch: 267 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Average loss per epoch\n",
    "ltrain_epochs = []\n",
    "for epoch_list in losses_train_epochs:\n",
    "  epoch_avg = sum(epoch_list) / len(train_loader)\n",
    "  ltrain_epochs.append(epoch_avg)\n",
    "\n",
    "ltest_epochs = []\n",
    "for epoch_list in losses_test_epochs:\n",
    "  epoch_avg = sum(epoch_list) / len(test_loader)\n",
    "  ltest_epochs.append(epoch_avg)\n",
    "\n",
    "# Print epoch achieving minimum vali loss\n",
    "min_value = min(ltest_epochs)\n",
    "min_index = ltest_epochs.index(min_value)\n",
    "\n",
    "print(\"The lowest loss:\", round(min_value,3), \"is found at epoch:\", min_index, \"\\n\")\n",
    "\n",
    "# Plot the losses over time\n",
    "plt.plot(ltrain_epochs, label=\"Training loss\")\n",
    "plt.plot(ltest_epochs, label=\"Validation loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Make lists of predictions and actual fractions (reference)\n",
    "nested_pred = best_pred # results from the epoch with the minimum loss is taken\n",
    "nested_actual = best_actual\n",
    "\n",
    "unnested_pred = []\n",
    "for data in nested_pred:\n",
    "  for batch in data:\n",
    "    for timestep in batch:\n",
    "      for target in range(len(timestep)):\n",
    "        unnested_pred.append(timestep[target])\n",
    "\n",
    "unnested_actual = []\n",
    "for data in nested_actual:\n",
    "  for batch in data:\n",
    "    for timestep in batch:\n",
    "      for target in range(len(timestep)):\n",
    "        unnested_actual.append(timestep[target])\n",
    "\n",
    "# These lists contain output for entire predictions \n",
    "# Now retain lists for each class\n",
    "pred_class = {}\n",
    "true_class = {}\n",
    "\n",
    "# Initialize lists for each class in predictions\n",
    "for i in range(len(targets)):\n",
    "  pred_class[f'{targets[i]}'] = unnested_pred[i::len(targets)]\n",
    "\n",
    "# Initialize lists for each class in reference data\n",
    "for i in range(len(targets)):\n",
    "  true_class[f'{targets[i]}'] = unnested_actual[i::len(targets)]\n",
    "\n",
    "RMSEavg = 0\n",
    "MAEavg = 0\n",
    "\n",
    "# Plot the lists as graphs\n",
    "\n",
    "# Loop through the data and plot the actual and predicted values\n",
    "for i in range(len(targets)):\n",
    "    # Get the data for the current class\n",
    "    true = true_class[f'{targets[i]}']\n",
    "    predicted = pred_class[f'{targets[i]}']\n",
    "\n",
    "    # Define the x-axis data as a range of values from 0 to the length of the data\n",
    "    x = range(len(true))\n",
    "\n",
    "    # Create a new figure\n",
    "    fig = plt.figure(i)\n",
    "\n",
    "    # Create a figure with certain size\n",
    "    fig = plt.figure(figsize=(20, 3))\n",
    "\n",
    "    # Create axes\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Plot the actual and predicted values\n",
    "    ax.plot(x, true, label='Actual')\n",
    "    ax.plot(x, predicted, label='Predicted')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Set the title using the class name\n",
    "    var_name = f'{targets[i]}'\n",
    "    ax.set_title(var_name)\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n",
    "\n",
    "    # Print RMSE / MAE\n",
    "    rmse = mean_squared_error(predicted, true) ** 0.5\n",
    "    RMSEavg = RMSEavg + rmse\n",
    "    print(f'RMSE for {var_name}: {round(rmse, 2)}')\n",
    "\n",
    "    difference = [abs(predicted - true) for predicted, true in zip(predicted, true)]\n",
    "    mae = mean(difference)\n",
    "    MAEavg = MAEavg + mae\n",
    "    print(f'MAE for {var_name}: {round(mae, 2)}')\n",
    "\n",
    "print(\"\\n\")\n",
    "RMSEavg = RMSEavg / len(targets)\n",
    "MAEavg = MAEavg / len(targets)\n",
    "\n",
    "print(f'Average RMSE is {round(RMSEavg, 2)} and average MAE is {round(MAEavg, 2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join predictions with IDs and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validf = LSTM_vali.copy() # either LSTM_vali or RF_vali\n",
    "\n",
    "# Add IDs (vali data was not shuffled so can add like this --- checked with excel!)\n",
    "import pandas as pd\n",
    "pred_df = validf.loc[:, ['sample_id', 'location_id', 'validation_id', 'reference_year', 'x', 'y']]\n",
    "\n",
    "# Adds predictions to df \n",
    "for i in range(len(targets)):\n",
    "  data = pred_class[f'{targets[i]}']\n",
    "  pred_df[targets[i]] = data \n",
    "\n",
    "# Show df\n",
    "pred_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('Output/PostLSTM/LSTM_PostLSTM_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
